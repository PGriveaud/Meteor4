{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhp2Kyn9drax6kEyblk2gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PGriveaud/Meteor4/blob/master/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozx1o2c-4ntY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, dim_input, dim_hid, dim_output, num_layers):\n",
        "    self.dim_input = dim_input\n",
        "    self.dim_hid = dim_hid\n",
        "    self.dim_output = dim_output\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.activations = []\n",
        "    self.hidden = []\n",
        "    self.weight = []\n",
        "    self.dimensions = [dim_input] + [dim_hid]*(num_layers-1) + [dim_output]   \n",
        "\n",
        "    self.gradient = []\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      w = np.random.rand(self.dimensions[i],self.dimensions[i+1])\n",
        "      #print(f\"weight {i} = {w}\")\n",
        "      self.weight.append(w)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def sigmoid_der(self, x):\n",
        "    return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "\n",
        "  def feedforward(self, input):\n",
        "    self.bias = []\n",
        "    self.activations.append(input)\n",
        "    self.hidden.append(0)  #The term h(1) is set as zero because it doesn't exist\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      b = np.random.rand(self.dimensions[i+1])\n",
        "      self.bias.append(b)\n",
        "      h = np.dot(self.activations[i], self.weight[i]) + self.bias[i]\n",
        "      a = self.sigmoid(h)\n",
        "      self.hidden.append(h)\n",
        "      self.activations.append(a)\n",
        "      \n",
        "      output = self.activations[-1] \n",
        "\n",
        "    return output\n",
        "\n",
        "  def error_1(self, y, y_hat):\n",
        "    J = y_hat - y \n",
        "    return J\n",
        "\n",
        "  def backpropagation(self, error):\n",
        "    for i in reversed(np.arange(1,self.num_layers+1)):\n",
        "      act = self.activations[i-1]\n",
        "      delt = error*self.sigmoid_der(self.hidden[i])\n",
        "      \n",
        "      act = act.reshape(act.shape[0],-1)\n",
        "      delt = delt.reshape(delt.shape[0],-1)\n",
        "\n",
        "      grad = np.dot(act, delt)\n",
        "      self.gradient.append(grad)\n",
        "      error = np.dot(delt,np.transpose(self.weight[i-1]))\n",
        "    return error\n",
        "\n",
        "  \n",
        "  def gradient_descent(self, lr):\n",
        "    self.gradient.reverse()\n",
        "    for i in range(len(self.weight)):\n",
        "      self.weight[i] = self.weight[i] - lr*self.gradient[i]\n",
        "\n",
        "    return self.weight\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfUcxFgTIQpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57d07e39-3d70-4699-a748-92b985d60279"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  input_size = 2\n",
        "  hidden_layers_size = 3\n",
        "  output_size = 1\n",
        "  layers_number = 2\n",
        "\n",
        "  mlp = MLP(input_size, hidden_layers_size, output_size, layers_number)\n",
        "\n",
        "  x = [5, 2]\n",
        "  x = np.array(x)\n",
        "  y = 0.5\n",
        "\n",
        "  n_epochs = 100\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  for i in np.arange(1,n_epochs):\n",
        "    y_hat = mlp.feedforward(x)\n",
        "    error = mlp.error_1(y,y_hat)\n",
        "    backprop = mlp.backpropagation(error)\n",
        "    train = mlp.gradient_descent(learning_rate)\n",
        "  print(f\"Error = {error}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error = [0.23808945]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YijDf8it39Gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqJsLT4GMgbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJjf35u9uG4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}